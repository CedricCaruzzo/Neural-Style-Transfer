{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846416bc-b292-4fb9-8e47-8a49abbb171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0f0df6-8766-46f2-b3da-aa25f196eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba14445-5224-473c-9220-72a25707c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image loading and preprocessing\n",
    "def image_loader(image_path, imsize):\n",
    "    loader = transforms.Compose([\n",
    "        transforms.Resize(imsize),\n",
    "        transforms.CenterCrop(imsize),\n",
    "        transforms.ToTensor()])\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "# Convert tensor to image for visualization\n",
    "def tensor_to_image(tensor):\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = transforms.ToPILImage()(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92df00dd-e2a5-4381-94cc-f0a93c1d7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content and Style losses\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.loss = nn.functional.mse_loss(x, self.target)\n",
    "        return x\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = self.gram_matrix(target_feature).detach()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        G = self.gram_matrix(x)\n",
    "        self.loss = nn.functional.mse_loss(G, self.target)\n",
    "        return x\n",
    "    \n",
    "    def gram_matrix(self, x):\n",
    "        batch_size, n_channels, height, width = x.size()\n",
    "        features = x.view(batch_size * n_channels, height * width)\n",
    "        G = torch.mm(features, features.t())\n",
    "        return G.div(batch_size * n_channels * height * width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff6bd48-358b-46e1-ac24-9c1f6ad62b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization class\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1).to(device)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1).to(device)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddcff4d5-4053-4850-9411-8dcc35d6808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get style model and losses\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                              style_img, content_img,\n",
    "                              content_layers=['conv_4'],\n",
    "                              style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']):\n",
    "    \n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    normalization = Normalization(normalization_mean, normalization_std)\n",
    "    \n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "    \n",
    "    model = nn.Sequential(normalization)\n",
    "    \n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = f'conv_{i}'\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = f'relu_{i}'\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = f'pool_{i}'\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = f'bn_{i}'\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        model.add_module(name, layer)\n",
    "        \n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "            \n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "    \n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "    \n",
    "    model = model[:(i + 1)]\n",
    "    \n",
    "    return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3263a02d-5f31-4f9c-bceb-43504b991d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style transfer function\n",
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                      content_img, style_img, input_img, num_steps=300,\n",
    "                      style_weight=500000, content_weight=1):\n",
    "    \n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "        normalization_mean, normalization_std, style_img, content_img)\n",
    "    \n",
    "    input_img.requires_grad_(True)\n",
    "    model.requires_grad_(False)\n",
    "    \n",
    "    optimizer = optim.LBFGS([input_img])\n",
    "    \n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "        \n",
    "        def closure():\n",
    "            with torch.no_grad():\n",
    "                input_img.clamp_(0, 1)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            \n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "            \n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "                \n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "            \n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "            \n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(f\"run {run[0]}:\")\n",
    "                print(f'Style Loss : {style_score.item():.4f} Content Loss: {content_score.item():.4f}')\n",
    "                \n",
    "            return style_score + content_score\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_img.clamp_(0, 1)\n",
    "        \n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0c690a-ebc7-40d6-b5fd-44340ba8dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main(content_path, style_path, imsize=512):\n",
    "    # Load images\n",
    "    content_img = image_loader(content_path, imsize)\n",
    "    style_img = image_loader(style_path, imsize)\n",
    "    input_img = content_img.clone()\n",
    "    \n",
    "    # Load VGG19 model\n",
    "    cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "    \n",
    "    # VGG normalization values\n",
    "    cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "    cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "    \n",
    "    # Run style transfer\n",
    "    output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                              content_img, style_img, input_img)\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(tensor_to_image(content_img))\n",
    "    plt.title('Content Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(tensor_to_image(style_img))\n",
    "    plt.title('Style Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(tensor_to_image(output))\n",
    "    plt.title('Output Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4778838-42fc-4658-b2eb-d7faef857d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda\\miniconda\\envs\\CellPaintingThesis\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\miniconda\\miniconda\\envs\\CellPaintingThesis\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\cedri\\AppData\\Local\\Temp\\ipykernel_6260\\3235729762.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mean = torch.tensor(mean).view(-1, 1, 1).to(device)\n",
      "C:\\Users\\cedri\\AppData\\Local\\Temp\\ipykernel_6260\\3235729762.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std = torch.tensor(std).view(-1, 1, 1).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 50:\n",
      "Style Loss : 54.0810 Content Loss: 48.9667\n",
      "run 100:\n",
      "Style Loss : 16.8145 Content Loss: 45.0795\n"
     ]
    }
   ],
   "source": [
    "main('img/content/village.jpg', 'img/style/vg_starry_night.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf284b-a786-4e5a-b069-23151de4b89d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
